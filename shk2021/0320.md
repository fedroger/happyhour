# Kubernetes Cluster Construction
## 1. Environmental preparation

### 1.1 VM installation
#### 1.1.1 Creating a Template VM
 recommended configuration parameters:
       Processor: >=2 CPUs
       Base Memory: 2048 MB
       Hard disk: >=20 GB
       OS: ubuntu-20.04.2.0
#### 1.1.2 Adding a NATNetwork
File --> Preferences --> Network

![01](https://github.com/fedroger/happyhour/blob/main/shk2021/01.png)

#### 1.1.3 Cloning three VMs based on Template VM
- Installing ssh on each VM
~~~ python
sudo apt update
sudo apt install openssh-server
sudo ufw allow ssh
~~~
- Setting a hostname and password for each VM (eg. bauen40)
~~~ python
sudo hostnamectl set-hostname HOSTNAME
sudo passwd root
~~~
- Modifying Network setting
![02](https://github.com/fedroger/happyhour/blob/main/shk2021/02.png)
- Configuring NATNetwork with IP
For example:
![03](https://github.com/fedroger/happyhour/blob/main/shk2021/03.png)
- Login on local machine to verify the ssh installation
![04](https://github.com/fedroger/happyhour/blob/main/shk2021/04.png)


### 1.2 Docker installation
Installation must be done on each VM
~~~ python
sudo apt-get install docker.io
vim /etc/docker/daemon.json
#add content as follows
{
  "exec-opts": ["native.cgroupdriver=systemd"]
}
# save and exit vim
systemctl daemon-reload
systemctl restart docker
# version of Docker on each VM must be identical!
~~~
### 1.3 Kubernetes installation
Installation must be done on each VM
~~~ python
sudo apt-get install curl -y
sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
sudo apt-get install kubeadm kubelet kubectl -y
# version of Docker on each VM must be identical!
# K8s' version must be compatible with the Docker's
~~~
## 2. Initialization of master node
~~~ python
sudo swapoff -a
kubeadm init --kubernetes-version=<VERSION OF K8s> --pod-network-cidr=10.244.0.0/16
# Network Plugins can be arbitrarily chosen, eg. Flannel, Weave, Calio. Here we use Flannel
~~~
After it you will see like this:
~~~ python
preflight                    Run pre-flight checks
certs                        Certificate generation
  /ca                          Generate the self-signed Kubernetes CA to provision identities for other Kubernetes components
  /apiserver                   Generate the certificate for serving the Kubernetes API
  /apiserver-kubelet-client    Generate the certificate for the API server to connect to kubelet
  /front-proxy-ca              Generate the self-signed CA to provision identities for front proxy
  /front-proxy-client          Generate the certificate for the front proxy client
  /etcd-ca                     Generate the self-signed CA to provision identities for etcd
  /etcd-server                 Generate the certificate for serving etcd
  /etcd-peer                   Generate the certificate for etcd nodes to communicate with each other
  /etcd-healthcheck-client     Generate the certificate for liveness probes to healthcheck etcd
  /apiserver-etcd-client       Generate the certificate the apiserver uses to access etcd
  /sa                          Generate a private key for signing service account tokens along with its public key
kubeconfig                   Generate all kubeconfig files necessary to establish the control plane and the admin kubeconfig file
  /admin                       Generate a kubeconfig file for the admin to use and for kubeadm itself
  /kubelet                     Generate a kubeconfig file for the kubelet to use *only* for cluster bootstrapping purposes
  /controller-manager          Generate a kubeconfig file for the controller manager to use
  /scheduler                   Generate a kubeconfig file for the scheduler to use
kubelet-start                Write kubelet settings and (re)start the kubelet
control-plane                Generate all static Pod manifest files necessary to establish the control plane
  /apiserver                   Generates the kube-apiserver static Pod manifest
  /controller-manager          Generates the kube-controller-manager static Pod manifest
  /scheduler                   Generates the kube-scheduler static Pod manifest
etcd                         Generate static Pod manifest file for local etcd
  /local                       Generate the static Pod manifest file for a local, single-node local etcd instance
upload-config                Upload the kubeadm and kubelet configuration to a ConfigMap
  /kubeadm                     Upload the kubeadm ClusterConfiguration to a ConfigMap
  /kubelet                     Upload the kubelet component config to a ConfigMap
upload-certs                 Upload certificates to kubeadm-certs
mark-control-plane           Mark a node as a control-plane
bootstrap-token              Generates bootstrap tokens used to join a node to a cluster
kubelet-finalize             Updates settings relevant to the kubelet after TLS bootstrap
  /experimental-cert-rotation  Enable kubelet client certificate rotation
addon                        Install required addons for passing Conformance tests
  /coredns                     Install the CoreDNS addon to a Kubernetes cluster
  /kube-proxy                  Install the kube-proxy addon to a Kubernetes cluster
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a Pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  /docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>
~~~
Network deployment
~~~ python
sudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
~~~

## 3. Joining worker nodes to the cluster
joining to the cluster using command as belows (use your own token):
~~~ python
sudo kubeadm join 192.168.1.190:6443 --token bzbwl4.ll5o9x3jjhqqwofa --discovery-token-ca-cert-hash sha256:ecb0223a05be3502c2d102f3e56104b10fcd105430eb723d3b3e816618323d73
~~~

Finally, you will see the cluster status using kubectl as follows:

![05](https://github.com/fedroger/happyhour/blob/main/shk2021/05.png)

## 4. creating images and pods
creating a local image using
~~~ python
docker build -t name:tag -f Dockerfilepath .
~~~

pushing image to docker hub

~~~ python
docker login
#username: fedshk2021
#pw:Fed123456
docker tag image-name username/image-name
docker push username/image-name
~~~

creating deployment
~~~ python
kubectl create deployment deployment-name --image=image-name
~~~

checking if the deployment and corresponding pod has been created
~~~ python
kubectl get deployment
kubectl get pod
~~~

![06](https://github.com/fedroger/happyhour/blob/main/shk2021/06.png)

![07](https://github.com/fedroger/happyhour/blob/main/shk2021/07.png)